DiskMultiMap:
DiskMultiMap is a hash table where each key points to multiple <value,context> pairs
The DiskMultiMap is stored on a disk file that is structured as described below:
-The file starts with a header that stores the number of buckets in the hash table, and positions to the last erased items from the file (to conserve space)
-Following that there are a number of offsets, pointing to the head KeyTuple (described below) in the list of keys
-The rest of the file contains KeyTuples and ValueContextTuples (described below) with data as well as pointers to the next data structure in their respective list
-Some KeyTuples and ValueContextTuples that have been erased contain the offset pointing to the next free position for storing data in place of their usual data

	KeyTuple (KT): Contains a constant size character array containing the key, offset pointing to the next KeyTuple with the same hash if one exists, offset pointing to the head ValueContextTuple
	ValueContextTuple (VCT): Contains constant size character arrays containing the value and context, and offset pointing to the next ValueContextTuple with the same key if one exists

	insert(const std::string& key, const std::string& value, const std::string& context):
		If the binary file isn't open or the input strings are too long, return false - O(1)
		Find a suitable offset for the new VCT in the file (reuse disk space if possible and update the header value for the last erased VCT position) - O(1)
		Create a new VCT with appropriate values and write it to the file at the offset found - O(1)
		Hash the key and search for it in all the keys with the same hash - O(N/B)
		If the key is found, add the new VCT to the end of the list of VCTs associated with that key - O(K)
		If the key isn't found:
			Find a suitable offset for the new KT in the file (reuse disk space if possible and update the header value for the last erased KT position) - O(1)
			If there is already a key with the same hash, link that KT to the new KT and write it to the file - O(1)
			Otherwise write the offset of the new KT to the position of the bucket containing it - O(1)
			Create a new KT with appropriate values and write it to the file at the offset found - O(1)
		Write any updates to the header
TIME COMPLEXITY: O(N/B + K)

	search(const std::string& key):
		Hash the key and search for that key in the list of keys with that hash - O(N/B)
		If the key isn't found, return the default (invalid) iterator
		Otherwise return an Iterator pointing to the first VCT containing a pointer to the binary file, and the key
TIME COMPLEXITY: O(N/B)

	erase(const std::string& key, const std::string& value, const std::string& context):
		[note: whenever a VCT or KT is deleted we update the list of open positions by adding another node to the list of removed VCTs or KTs]
		First find the position of the key by hashing the key and looking through the list of KTs in the appropriate bucket - O(N/B)
		If the key couldn't be found, return 0
		Otherwise, delete elements from the start of the list of VCTs and erase and link the KT to the next VCT until we find a VCT that doesn't match - O(K)
		If the entire list has been erased, update the previous KT or the bucket to point the the KT after this KT - O(1)
		If there are still nodes remaining in the VCT list, we go through them, erase matches and link the previous node to the next if a match is found - O(K)
TIME COMPLEXITY: O(N/B + K)

---------------------------------------------

DiskMultiMap::Iterator:
Each iterator does caching so that unless the iterator is updated, it doesn't read the binary file more than once. It accomplishes this using a boolean that is false when the iterator is first created and whenever it is updated (whenever operator++() is called for example).
Iterators also store the offset of the value it's looking at in the binary file, the key that the value is associated with, as well as a pointer to the binary file.
	operator++():
		Read the value at the offset and set the offset to the next offset in the list
TIME COMPLEXITY: O(1)
	operator*():
		Read the value at the offset and convert it to a MultiMapTuple that can is returned
TIME COMPLEXITY: O(1)

---------------------------------------------

IntelWeb:
Contains DiskMultiMaps (hash multi-maps stored on disk) initiator and target events that store mapping from initiator to all its targets and vice versa respectively.
	ingest(const std::string& telemetryFile):
		[note: If any operation in this function fails (eg. reading/writing/opening file), return false without proceeding further]
		Open the telemetry file
		While there are still lines in the file, get the line: - O(T)
			Get the context, initiator, and target from the file and insert the mappings the the initiator and target events DiskMultiMaps - O(1)
TIME COMPLEXITY: O(T) - T = number of lines of telemetry data

	crawl(const std::vector<std::string>& indicators, unsigned int minPrevalenceToBeGood, std::vector<std::string>& badEntitiesFound, std::vector<InteractionTuple>& interactions):
	DATA STRUCTURES:
		-hash map: stores state = whether entity shouldn't be processed (0), needs to be processed [and is an initiator (4)] or [and isn't initiator (1)], or has already been processed [and isn't popular (2)] or [and is popular (3)] 
		-queue: stores bad entities that need to be processed (ie. searched for associations)
		-set: stores all the bad interactions which can be easily retrieved in sorted order from a set.
	ALGORITHM:
		Add each indicator of the queue of badEntitiesToBeProcessed and set their state to 4
		While there are still entities that need to be processed:
			Get and pop the key from the front of the queue
			Check the number of initiator and target associations that the key has
			If the number of associations is >= the minimum prevalence to be good, and the entity isn't an initiator, then set its state to 3 to go to the next entity in the queue
			Add the entity to the badEntitiesFound, set its state to 2, and increment the number of bad entities
			For all associations, if the value hasn't been process yet, set its state to 1 and add it to the queue. Also, add that interation to the set
		Sort all the badEntitiesFound, push the interactions from the set to the vector, and return the number of bad entities
TIME COMPLEXITY: O(TlogT) - T = number of bad interactions

	purge(const std::string& entity):
		For all initiator associations of the entity, erase it from the initiator events DiskMultiMap and the reverse from the target events DiskMultiMap
		For all target associations of the entity, erase it from the target events DiskMultiMap and the reverse from the initiator events DiskMultiMap
		Return whether at least one association was deleted (ie, if it went through at least one loop)
TIME COMPLEXITY: O(M) - M = number of associations deleted
